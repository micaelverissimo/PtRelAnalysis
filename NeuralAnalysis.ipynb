{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authors: Isabella Silva Ferreira and Micael Veríssimo de Araújo\n",
    "# e-mails: (is_bella21@hotmail.com)    (micaelvero@hotmail.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "import theano\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Right!\n"
     ]
    }
   ],
   "source": [
    "# Read data File and sapare the all the sets\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n",
    "import random\n",
    "\n",
    "print \"All Right!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191100, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.52196307e-03,   6.12077856e-05,   1.65209174e-02, ...,\n",
       "          9.21870469e+01,   5.98104895e+01,   6.10195977e+01],\n",
       "       [  1.67018827e-02,   1.13179535e-01,   9.47747454e-02, ...,\n",
       "          1.07602320e+02,   5.98104895e+01,   6.44654687e+01],\n",
       "       [  8.95111356e-03,   9.14592529e-05,   8.82276613e-03, ...,\n",
       "          3.70010500e+02,   4.97765697e+01,   1.06724656e+02],\n",
       "       ..., \n",
       "       [  1.62160210e-02,   5.48769441e-03,   2.28184238e-02, ...,\n",
       "          3.86259570e+01,   3.57536859e+01,   3.39022539e+01],\n",
       "       [  1.42509257e-02,   1.21706940e-01,   4.58930284e-02, ...,\n",
       "          1.07292875e+02,   5.50991584e+01,   9.68779375e+01],\n",
       "       [  1.58070903e-02,   4.79239076e-02,   1.20003656e-01, ...,\n",
       "          6.82103906e+01,   5.50991584e+01,   5.35117266e+01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm our array shape\n",
    "data = np.load(\"/home/micael/MyWorkspace/DATA/lvbb125Array.npy\")\n",
    "data_target = data[:,[12]]\n",
    "\n",
    "data_inputs = data[:,:12]\n",
    "print data.shape\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191100, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.52196307e-03,   6.12077856e-05,   1.65209174e-02, ...,\n",
       "         -1.83297706e+00,   9.21870469e+01,   5.98104895e+01],\n",
       "       [  1.67018827e-02,   1.13179535e-01,   9.47747454e-02, ...,\n",
       "          1.78017831e+00,   1.07602320e+02,   5.98104895e+01],\n",
       "       [  8.95111356e-03,   9.14592529e-05,   8.82276613e-03, ...,\n",
       "          5.18436611e-01,   3.70010500e+02,   4.97765697e+01],\n",
       "       ..., \n",
       "       [  1.62160210e-02,   5.48769441e-03,   2.28184238e-02, ...,\n",
       "         -2.27387905e+00,   3.86259570e+01,   3.57536859e+01],\n",
       "       [  1.42509257e-02,   1.21706940e-01,   4.58930284e-02, ...,\n",
       "         -1.46886933e+00,   1.07292875e+02,   5.50991584e+01],\n",
       "       [  1.58070903e-02,   4.79239076e-02,   1.20003656e-01, ...,\n",
       "         -3.02542996e+00,   6.82103906e+01,   5.50991584e+01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data_inputs.shape\n",
    "data_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  61.01959766],\n",
       "       [  64.46546875],\n",
       "       [ 106.72465625],\n",
       "       ..., \n",
       "       [  33.90225391],\n",
       "       [  96.8779375 ],\n",
       "       [  53.51172656]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data_target.shape\n",
    "data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/home/micael/MyWorkspace/DATA/NN_outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Process for 1 Fold\n"
     ]
    }
   ],
   "source": [
    "# Train NN\n",
    "#from Functions import LogFunctions as log\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n",
    "import random\n",
    "\n",
    "\n",
    "# Train information \n",
    "n_folds = 2\n",
    "n_inits = 1\n",
    "norm = 'mapstd'\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['norm'] = norm\n",
    "\n",
    "trn_desc = {}\n",
    "\n",
    "# Slip the train and test sets\n",
    "test_id= random.sample(range(data_inputs.shape[0]),int(np.floor(0.1*data_target.shape[0])))\n",
    "lista_id = np.asarray(range(0, data_inputs.shape[0]))\n",
    "all_id = np.ndarray.tolist(lista_id)\n",
    "train_id = list(set(lista_id) - set(test_id))\n",
    "\n",
    "                           \n",
    "\n",
    "for ifold in range(0,10):\n",
    "#    train_id, test_id = CVO[ifold]\n",
    "    \n",
    "    # normalize data based in train set\n",
    "    if train_info['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(data_inputs[train_id,:])\n",
    "    elif train_info['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(data_inputs[train_id,:])\n",
    "    elif train_info['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(data_inputs[train_id,:])\n",
    "        \n",
    "    norm_all_data = scaler.transform(data_inputs)\n",
    "    \n",
    "# Normalize the outputs\n",
    "    [freq, target_values] = np.histogram(data_target[train_id],\n",
    "             bins=np.linspace(data_target[train_id].min(),data_target[train_id].max(),50))\n",
    "\n",
    "    MOP = target_values[np.argmax(freq)]\n",
    "    \n",
    "    norm_target = data_target/MOP\n",
    "\n",
    "    print 'Train Process for %i Fold'%(ifold+1)\n",
    "    \n",
    "    best_init = 0\n",
    "    best_loss = 999\n",
    "    \n",
    "    for i_init in range(0,10):\n",
    "        #print 'Init: %i of %i'%(i_init,train_info['n_inits'])\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=data_inputs.shape[1], init='uniform'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Dense(data_target.shape[1], init='uniform')) \n",
    "        model.add(Activation('linear'))\n",
    "        \n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=sgd\n",
    "                      ,metrics=['accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=25, \n",
    "                                            verbose=0, mode='auto')\n",
    "        \n",
    "        modelCheckPoint = callbacks.ModelCheckpoint(path+\"/\"+\"Modelv4.h5\", monitor='val_loss',\n",
    "                                                    verbose=0, save_best_only=True, mode='auto')\n",
    "        #start_time = time.time()\n",
    "        init_trn_desc = model.fit(norm_all_data[train_id], norm_target[train_id], \n",
    "                                nb_epoch=500, \n",
    "                                batch_size=8,\n",
    "                                callbacks=[earlyStopping, modelCheckPoint], \n",
    "                                verbose=0,\n",
    "                                validation_data=(norm_all_data[test_id],norm_target[test_id]),\n",
    "                                shuffle=True)\n",
    "\n",
    "        \n",
    "    \n",
    "    trn_desc[ifold] = init_trn_desc  \n",
    "\n",
    "\n",
    "print \"All Right!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = \"/home/micael/MyWorkspace/DATA/NN_outputs\"\n",
    "print \"Output path define\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model, train info and train description\n",
    "\n",
    "\n",
    "\n",
    "model.save(output+\"/\"+\"Model2.h5\")\n",
    "\n",
    "joblib.dump(init_trn_desc.history,output+\"/\"+\"Train_desc2.jbl\",compress=9)\n",
    "joblib.dump([train_info],output+\"/\"+\"Train_info2.jbl\",compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(init_trn_desc.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the NN output with all envents\n",
    "\n",
    "NN_output = model.predict(norm_all_data, batch_size=8, verbose=0)\n",
    "\n",
    "print NN_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "plt.plot(init_trn_desc.history['acc'])\n",
    "plt.plot(init_trn_desc.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(init_trn_desc.history['loss'])\n",
    "plt.plot(init_trn_desc.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(data_target, 60)\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.xlabel(\"Pt Truth\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(norm_target, 60)\n",
    "plt.title(\"Normalized Target Distribution\")\n",
    "plt.xlabel(\"Pt Truth / MOP(Pt Truth)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(NN_output, 60)\n",
    "plt.title(\"NN output\")\n",
    "plt.xlabel(\"Pt Truth\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist((norm_target - NN_output)/norm_target, 60)\n",
    "plt.title(\"Target - NN output / Target\")\n",
    "plt.xlabel(\"Pt Truth\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model(output+\"/\"+\"Model2.h5\")\n",
    "\n",
    "test_id= random.sample(range(data_inputs.shape[0]),int(np.floor(0.1*data_target.shape[0])))\n",
    "lista_id = np.asarray(range(0, data_inputs.shape[0]))\n",
    "all_id = np.ndarray.tolist(lista_id)\n",
    "train_id = list(set(lista_id) - set(test_id))\n",
    "\n",
    "# normalize data based in train set\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data_inputs[train_id,:])        \n",
    "norm_all_data = scaler.transform(data_inputs)\n",
    "    \n",
    "# Normalize the outputs\n",
    "[freq, target_values] = np.histogram(data_target[train_id],\n",
    "            bins=np.linspace(data_target[train_id].min(),data_target[train_id].max(),50))\n",
    "\n",
    "MOP = target_values[np.argmax(freq)]\n",
    "    \n",
    "norm_target = data_target/MOP\n",
    "\n",
    "NN_output = model.predict(norm_all_data, batch_size=8, verbose=0)\n",
    "print NN_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_output_noNorm = NN_output * MOP\n",
    "np.save(output+\"/\"+\"CorFactors.npy\", NN_output_noNorm)\n",
    "print NN_output_noNorm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
