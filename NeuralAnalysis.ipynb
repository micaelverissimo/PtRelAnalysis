{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Authors: Isabella Silva Ferreira and Micael Veríssimo de Araújo\n",
    "# e-mails: (is_bella21@hotmail.com)    (micaelvero@hotmail.com)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Right!\n"
     ]
    }
   ],
   "source": [
    "# Read data File and sapare the all the sets\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n",
    "import random\n",
    "\n",
    "print \"All Right!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191100, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.52196307e-03,   6.12077856e-05,   1.65209174e-02, ...,\n",
       "          9.21870469e+01,   5.98104895e+01,   6.10195977e+01],\n",
       "       [  1.67018827e-02,   1.13179535e-01,   9.47747454e-02, ...,\n",
       "          1.07602320e+02,   5.98104895e+01,   6.44654687e+01],\n",
       "       [  8.95111356e-03,   9.14592529e-05,   8.82276613e-03, ...,\n",
       "          3.70010500e+02,   4.97765697e+01,   1.06724656e+02],\n",
       "       ..., \n",
       "       [  1.62160210e-02,   5.48769441e-03,   2.28184238e-02, ...,\n",
       "          3.86259570e+01,   3.57536859e+01,   3.39022539e+01],\n",
       "       [  1.42509257e-02,   1.21706940e-01,   4.58930284e-02, ...,\n",
       "          1.07292875e+02,   5.50991584e+01,   9.68779375e+01],\n",
       "       [  1.58070903e-02,   4.79239076e-02,   1.20003656e-01, ...,\n",
       "          6.82103906e+01,   5.50991584e+01,   5.35117266e+01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm our array shape\n",
    "path = \"/home/isabella/Workspace/python/PtRelAnalysis/PtRelAnalysis/Results/DataFiles\"\n",
    "data = np.load(path+\"/\"+\"lvbb125Array.npy\")\n",
    "data_target = data[:,[12]]\n",
    "\n",
    "data_inputs = data[:,:12]\n",
    "print data.shape\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191100, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  9.52196307e-03,   6.12077856e-05,   1.65209174e-02, ...,\n",
       "         -1.83297706e+00,   9.21870469e+01,   5.98104895e+01],\n",
       "       [  1.67018827e-02,   1.13179535e-01,   9.47747454e-02, ...,\n",
       "          1.78017831e+00,   1.07602320e+02,   5.98104895e+01],\n",
       "       [  8.95111356e-03,   9.14592529e-05,   8.82276613e-03, ...,\n",
       "          5.18436611e-01,   3.70010500e+02,   4.97765697e+01],\n",
       "       ..., \n",
       "       [  1.62160210e-02,   5.48769441e-03,   2.28184238e-02, ...,\n",
       "         -2.27387905e+00,   3.86259570e+01,   3.57536859e+01],\n",
       "       [  1.42509257e-02,   1.21706940e-01,   4.58930284e-02, ...,\n",
       "         -1.46886933e+00,   1.07292875e+02,   5.50991584e+01],\n",
       "       [  1.58070903e-02,   4.79239076e-02,   1.20003656e-01, ...,\n",
       "         -3.02542996e+00,   6.82103906e+01,   5.50991584e+01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data_inputs.shape\n",
    "data_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  61.01959766],\n",
       "       [  64.46546875],\n",
       "       [ 106.72465625],\n",
       "       ..., \n",
       "       [  33.90225391],\n",
       "       [  96.8779375 ],\n",
       "       [  53.51172656]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print data_target.shape\n",
    "data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK! 46.725858239\n"
     ]
    }
   ],
   "source": [
    "# Train NN\n",
    "#from Functions import LogFunctions as log\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n",
    "import random\n",
    "\n",
    "\n",
    "test_id= random.sample(range(data_inputs.shape[0]),int(np.floor(0.1*data_target.shape[0])))\n",
    "lista_id = np.asarray(range(0, data_inputs.shape[0]))\n",
    "all_id = np.ndarray.tolist(lista_id)\n",
    "train_id = list(set(lista_id) - set(test_id))\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data_inputs[train_id,:])\n",
    "\n",
    "[freq, target_values] = np.histogram(data_target[train_id],\n",
    "             bins=np.linspace(data_target[train_id].min(),data_target[train_id].max(),50))\n",
    "\n",
    "MOP = target_values[np.argmax(freq)]\n",
    "\n",
    "print \"OK!\", MOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train NN\n",
    "#from Functions import LogFunctions as log\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n",
    "import random\n",
    "\n",
    "\n",
    "# Train information \n",
    "n_folds = 2\n",
    "n_inits = 1\n",
    "norm = 'mapstd'\n",
    "\n",
    "train_info = {}\n",
    "train_info['n_folds'] = n_folds\n",
    "train_info['n_inits'] = n_inits\n",
    "train_info['norm'] = norm\n",
    "\n",
    "trn_desc = {}\n",
    "\n",
    "                           \n",
    "for ifold in range(0,10):\n",
    "#    train_id, test_id = CVO[ifold]\n",
    "\n",
    "    # Slip the train and test sets\n",
    "    test_id= random.sample(range(data_inputs.shape[0]),int(np.floor(0.1*data_target.shape[0])))\n",
    "    lista_id = np.asarray(range(0, data_inputs.shape[0]))\n",
    "    all_id = np.ndarray.tolist(lista_id)\n",
    "    train_id = list(set(lista_id) - set(test_id))\n",
    "    \n",
    "    # normalize data based in train set\n",
    "    if train_info['norm'] == 'mapstd':\n",
    "        scaler = preprocessing.StandardScaler().fit(data_inputs[train_id,:])\n",
    "    elif train_info['norm'] == 'mapstd_rob':\n",
    "        scaler = preprocessing.RobustScaler().fit(data_inputs[train_id,:])\n",
    "    elif train_info['norm'] == 'mapminmax':\n",
    "        scaler = preprocessing.MinMaxScaler().fit(data_inputs[train_id,:])\n",
    "        \n",
    "    norm_all_data = scaler.transform(data_inputs)\n",
    "    \n",
    "# Normalize the outputs\n",
    "    [freq, target_values] = np.histogram(data_target[train_id],\n",
    "             bins=np.linspace(data_target[train_id].min(),data_target[train_id].max(),50))\n",
    "\n",
    "    MOP = target_values[np.argmax(freq)]\n",
    "    \n",
    "    norm_target = data_target/MOP\n",
    "\n",
    "    print 'Train Process for %i Fold'%(ifold+1)\n",
    "    \n",
    "    best_init = 0\n",
    "    best_loss = 999\n",
    "    \n",
    "    for i_init in range(0,10):\n",
    "        #print 'Init: %i of %i'%(i_init,train_info['n_inits'])\n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=data_inputs.shape[1], init='uniform'))\n",
    "        model.add(Activation('tanh'))\n",
    "        model.add(Dense(data_target.shape[1], init='uniform')) \n",
    "        model.add(Activation('linear'))\n",
    "        \n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer=sgd\n",
    "                      ,metrics=['accuracy'])\n",
    "\n",
    "        # Train model\n",
    "        earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=25, \n",
    "                                            verbose=0, mode='auto')\n",
    "        modelCheckPoint = callbacks.ModelCheckpoint(path+\"/\"+\"Modelv4.h5\", monitor='val_loss',\n",
    "                                                    verbose=0, save_best_only=True, mode='min')\n",
    "        #start_time = time.time()\n",
    "        init_trn_desc = model.fit(norm_all_data[train_id], norm_target[train_id], \n",
    "                                nb_epoch=500, \n",
    "                                batch_size=8,\n",
    "                                callbacks=[earlyStopping, modelCheckPoint], \n",
    "                                verbose=0,\n",
    "                                validation_data=(norm_all_data[test_id],norm_target[test_id]),\n",
    "                                shuffle=True)\n",
    "\n",
    "        \n",
    "    \n",
    "    trn_desc[ifold] = init_trn_desc  \n",
    "    \n",
    "model.save(path+\"/\"+\"Modelv5.h5\")\n",
    "\n",
    "joblib.dump(init_trn_desc.history,output+\"/\"+\"Train_desc5.jbl\",compress=9)\n",
    "joblib.dump([train_info],output+\"/\"+\"Train_info5.jbl\",compress=9)\n",
    "\n",
    "print \"All Right!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.34583414]\n",
      " [ 1.2680223 ]\n",
      " [ 2.19578695]\n",
      " ..., \n",
      " [ 0.81697512]\n",
      " [ 2.22765589]\n",
      " [ 1.22342014]]\n"
     ]
    }
   ],
   "source": [
    "norm_all_data = scaler.transform(data_inputs)\n",
    "NN_output = model.predict(norm_all_data, batch_size=8, verbose=0)\n",
    "\n",
    "print NN_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1695b00c00e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minit_trn_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Train_desc5.jbl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_trn_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_trn_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "\n",
    "init_trn_desc = joblib.load(path+\"/\"+\"Train_desc5.jbl\")\n",
    "plt.plot(init_trn_desc.history['acc'])\n",
    "plt.plot(init_trn_desc.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(init_trn_desc.history['loss'])\n",
    "plt.plot(init_trn_desc.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(data_target, 60)\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.xlabel(\"Pt Truth\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(norm_target, 60)\n",
    "plt.title(\"Normalized Target Distribution\")\n",
    "plt.xlabel(\"Pt Truth / MOP(Pt Truth)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist(NN_output, 60)\n",
    "plt.title(\"NN output\")\n",
    "plt.xlabel(\"Pt Truth\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.hist((norm_target - NN_output)/norm_target, 60)\n",
    "plt.title(\"Target - NN output / Target\")\n",
    "plt.xlabel(\"Pt Truth\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train NN\n",
    "#from Functions import LogFunctions as log\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model=load_model(path+\"/\"+\"Modelv4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j1 (12,)\n",
      "j2 (12,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isabella/.virtualenvs/pt_rel_analysis/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/home/isabella/.virtualenvs/pt_rel_analysis/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error when checking : expected dense_input_2 to have shape (None, 12) but got array with shape (12, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9a17f4d0c1fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mnorm_j2_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_j2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mPt1_cor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_j1_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mPt2_cor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_j2_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isabella/.virtualenvs/pt_rel_analysis/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isabella/.virtualenvs/pt_rel_analysis/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1177\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[1;32m   1178\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m                                    check_batch_dim=False)\n\u001b[0m\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/isabella/.virtualenvs/pt_rel_analysis/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m    109\u001b[0m                                         \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                                         \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                                         str(array.shape))\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error when checking : expected dense_input_2 to have shape (None, 12) but got array with shape (12, 1)"
     ]
    }
   ],
   "source": [
    "# Apply the correction for each jet\n",
    "from keras.models import load_model\n",
    "import ROOT\n",
    "import copy\n",
    "#from NeuralNetwork import MultiLayerPerceptron as mlp\n",
    "from functions import HistogramFunctions,AuxiliarFunctions,FigureFunctions,FitFunctions,rootnotes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ROOT.gROOT.Reset()\n",
    "ROOT.gStyle.SetOptStat(0)\n",
    "#print \"Everthing Ok\"\n",
    "\n",
    "output = \"/home/isabella/Workspace/python/PtRelAnalysis/PtRelAnalysis/Results/Plots\"\n",
    "\n",
    "path = \"/home/isabella/Workspace/python/PtRelAnalysis/PtRelAnalysis/Results/DataFiles\"\n",
    "\n",
    "#CorFac = np.load(input_Cor+\"/\"+\"CorFactors.npy\")\n",
    "\n",
    "#model=load_model(path+\"/\"+\"Modelv4.h5\")\n",
    "\n",
    "data_input = \"/home/isabella/Workspace/Adrian2015/Public/WH/output/local/readPaul_1_0_J1Pt45+2BTag+TruthGENWZ+Clean_1_perevent+perjet/lvbb125.root\"\n",
    "treeName = \"perevent\"\n",
    "\n",
    "\n",
    "file = ROOT.TFile(data_input,\"READ\")\n",
    "tree = file.Get(treeName)\n",
    "\n",
    "#nrEvents = tree.GetEntries()\n",
    "nrEvents = 9\n",
    "\n",
    "# for histograms\n",
    "bins = 60\n",
    "m_max = 300\n",
    "m_min = 0\n",
    "\n",
    "ylabel = \"Occurrences\"\n",
    "\n",
    "# Use Colors\n",
    "list_color = []\n",
    "list_color.append(ROOT.kBlue) # For truth\n",
    "list_color.append(ROOT.kRed) \n",
    "list_color.append(ROOT.kGreen)\n",
    "    #list_color.append(ROOT.kRed+2)\n",
    "    #list_color.append(ROOT.kRed+3)\n",
    "    #list_color.append(ROOT.kRed+4)\n",
    "    #end list of colors\n",
    "\n",
    "#  Make a list of mass histogram\n",
    "list_inf_M = []\n",
    "\n",
    "hist_inf = HistogramFunctions.OneDimHistInfo(\"Truth Mass \",\"h_lvbb_M_truth\",bins,m_min,m_max,\"Invariant Mass\",ylabel)\n",
    "list_inf_M.append(hist_inf)\n",
    "\n",
    "hist_inf = HistogramFunctions.OneDimHistInfo(\"EMJESGSCMu Mass \",\"h_lvbb_M_reco\",bins,m_min,m_max,\"Invariant Mass\",ylabel)\n",
    "list_inf_M.append(hist_inf)\n",
    "\n",
    "hist_inf = HistogramFunctions.OneDimHistInfo(\"NN Mass \",\"h_lvbb_M_NN\",bins,m_min,m_max,\"Invariant Mass\",ylabel)\n",
    "list_inf_M.append(hist_inf)\n",
    "\n",
    "list_hist_M = HistogramFunctions.CreateListOf1DHistograms(list_inf_M,list_color)\n",
    "\n",
    "\n",
    "\n",
    "c = rootnotes.canvas(\"c\", (800, 600))\n",
    "#h = ROOT.TH1F(\"Mass\",\"Mass Distribution\",60,0,300)\n",
    "\n",
    "Corrections = [\"GENWZ\",\"EMJESGSCMu\"]\n",
    "for (j,Correction) in enumerate(Corrections):   \n",
    "    for (i,entry) in enumerate(tree):\n",
    "        if Correction == \"GENWZ\":\n",
    "            list_hist_M[0].Fill(getattr(entry,\"j1j2_\"+Correction+\"_M\"))\n",
    "        else:\n",
    "            list_hist_M[1].Fill(getattr(entry,\"j1j2_\"+Correction+\"_M\"))\n",
    "        \n",
    "    for (i,entry) in enumerate(tree):\n",
    "        if nrEvents>0:\n",
    "            if i>nrEvents:\n",
    "                break\n",
    "        if Correction == \"GENWZ\":\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            j1_LV = ROOT.TLorentzVector()\n",
    "            j2_LV = ROOT.TLorentzVector()\n",
    "            \n",
    "            #=============================================================================\n",
    "            #=======================EMJESGSCMu========================================\n",
    "            # variables for EMJESGSCMu j1.     \n",
    "            E1_EMJESGSCMu = tree.j1_EMJESGSCMu_E\n",
    "            Phi1_EMJESGSCMu = tree.j1_EMJESGSCMu_Phi\n",
    "            Eta1_EMJESGSCMu = tree.j1_EMJESGSCMu_Eta\n",
    "            Pt1_EMJESGSCMu = tree.j1_EMJESGSCMu_Pt\n",
    "    \n",
    "            # variables for EMJESGSCMu j2.     \n",
    "            E2_EMJESGSCMu = tree.j2_EMJESGSCMu_E\n",
    "            Phi2_EMJESGSCMu = tree.j2_EMJESGSCMu_Phi\n",
    "            Eta2_EMJESGSCMu = tree.j2_EMJESGSCMu_Eta\n",
    "            Pt2_EMJESGSCMu = tree.j2_EMJESGSCMu_Pt\n",
    "    \n",
    "            # PtRel in EMJESGSCMu\n",
    "            Theta1_EMJESGSCMu = AuxiliarFunctions.theta(Eta1_EMJESGSCMu)\n",
    "            Comp1_EMJESGSCMu = AuxiliarFunctions.Componets(E1_EMJESGSCMu,Eta1_EMJESGSCMu,Phi1_EMJESGSCMu,Theta1_EMJESGSCMu)\n",
    "            norm1_EMJESGSCMu = AuxiliarFunctions.norm(Comp1_EMJESGSCMu)\n",
    "    \n",
    "            Theta2_EMJESGSCMu = AuxiliarFunctions.theta(Eta2_EMJESGSCMu)\n",
    "            Comp2_EMJESGSCMu = AuxiliarFunctions.Componets(E2_EMJESGSCMu,Eta2_EMJESGSCMu,Phi2_EMJESGSCMu,Theta2_EMJESGSCMu)\n",
    "            norm2_EMJESGCMu = AuxiliarFunctions.norm(Comp2_EMJESGSCMu)\n",
    "    \n",
    "            # sum of components.\n",
    "            Comp3_EMJESGSCMu = AuxiliarFunctions.somaComponents(Comp1_EMJESGSCMu,Comp2_EMJESGSCMu)\n",
    "            norm3_EMJESGSCMu = AuxiliarFunctions.norm(Comp3_EMJESGSCMu)\n",
    "            numerador1_EMJESGSCMu = AuxiliarFunctions.numerador(Comp1_EMJESGSCMu,Comp3_EMJESGSCMu)    \n",
    "            angle1_EMJESGSCMu = AuxiliarFunctions.alpha(numerador1_EMJESGSCMu,norm1_EMJESGSCMu,norm3_EMJESGSCMu)\n",
    "    \n",
    "            #compute Relative Pt\n",
    "            PtRel_j1_EMJESGSCMu = AuxiliarFunctions.PtRelative(E1_EMJESGSCMu,angle1_EMJESGSCMu)\n",
    "    \n",
    "            j1_FracEM3 = tree.j1_FracEM3\n",
    "            j1_FracTile0 = tree.j1_FracTile0\n",
    "            j1_TrkWidth = tree.j1_TrkWidth\n",
    "            j1_EMF = tree.j1_EMF\n",
    "            j1_JVF = tree.j1_JVF\n",
    "            j1_NTrk = tree.j1_NTrk \n",
    "            j1_SumPtTrk = tree.j1_SumPtTrk\n",
    "            # only for EM and j2.\n",
    "            j2_FracEM3 = tree.j2_FracEM3\n",
    "            j2_FracTile0 = tree.j2_FracTile0\n",
    "            j2_TrkWidth = tree.j2_TrkWidth\n",
    "            j2_EMF = tree.j2_EMF\n",
    "            j2_JVF = tree.j2_JVF\n",
    "            j2_NTrk = tree.j2_NTrk \n",
    "            j2_SumPtTrk = tree.j2_SumPtTrk\n",
    "    \n",
    "            # Create a numpy array with all data and normalize then\n",
    "            j1 = np.array([j1_FracEM3, j1_FracTile0, j1_TrkWidth, j1_EMF, j1_JVF, j1_NTrk,\n",
    "                              j1_SumPtTrk, Pt1_EMJESGSCMu, Eta1_EMJESGSCMu, Phi1_EMJESGSCMu, \n",
    "                              E1_EMJESGSCMu, PtRel_j1_EMJESGSCMu])\n",
    "            \n",
    "            V_j1 = np.reshape(j1, (1,12))\n",
    "            \n",
    "            print \"j1\", V_j1.shape\n",
    "            \n",
    "            norm_j1_data = scaler.transform(V_j1)\n",
    "            \n",
    "            \n",
    "            j2 = np.array([j2_FracEM3, j2_FracTile0, j2_TrkWidth, j2_EMF, j2_JVF, j2_NTrk,\n",
    "                             j2_SumPtTrk, Pt2_EMJESGSCMu, Eta2_EMJESGSCMu, Phi2_EMJESGSCMu,\n",
    "                             E2_EMJESGSCMu, PtRel_j1_EMJESGSCMu])\n",
    "            \n",
    "            V_j2 = np.reshape(j2, (1,12))\n",
    "            print \"j2\", V_j2.shape\n",
    "            \n",
    "            norm_j2_data = scaler.transform(V_j2)\n",
    "            \n",
    "            Pt1_cor = model.predict(norm_j1_data, batch_size=8, verbose=0)\n",
    "            Pt2_cor = model.predict(norm_j2_data, batch_size=8, verbose=0)\n",
    "            \n",
    "            E1_cor = (Pt1_cor/Pt1_EMJESGSCMu)*E1_EMJESGSCMu\n",
    "            E2_cor = (Pt2_cor/Pt2_EMJESGSCMu)*E2_EMJESGSCMu\n",
    "            \n",
    "            j1_LV.SetPtEtaPhiE(Pt1_cor,Eta1_EMJESGSCMu,Phi1_EMJESGSCMu,E1_cor)\n",
    "            j2_LV.SetPtEtaPhiE(Pt2_cor,Eta2_EMJESGSCMu,Phi2_EMJESGSCMu,E2_cor)\n",
    "            \n",
    "            dijet_LV = j1_LV + j2_LV\n",
    "            Mass = dijet_LV.M()\n",
    "            list_hist_M[0].Fill(Mass)\n",
    "            del V_j1\n",
    "            del V_j2\n",
    "                \n",
    "            \n",
    "clone_list_M = []\n",
    "for hist in list_hist_M:\n",
    "    newhist = hist.Clone()\n",
    "    clone_list_M.append(newhist)\n",
    "\n",
    "HistogramFunctions.Draw1DHists(list_hist_M,output)\n",
    "HistogramFunctions.DrawList1DHistInCanvas(clone_list_M, \"Mass Plot\", \" Invariant Mass \", \"Events\", c)        \n",
    "\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
